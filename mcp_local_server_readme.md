# MCP-LOCAL-SERVER

Servidor **MCP** local en **Python** con herramientas para:

* `pdf_extract`: extrae **texto** y **tablas** desde PDFs locales
* `data_profile`: perfila **CSV** (filas, columnas, memoria, preview)
* `ts_forecast`: pronÃ³stico **simple** de series temporales con fecha
* `report_generate`: genera **reportes HTML y PDF** (tablas + grÃ¡ficos)
* `llm_chat`: chat con **Llama** vÃ­a **Ollama** (endpoint OpenAI-compatible)

Incluye un **cliente demo** (`demo.py`) que orquesta todo y un **chat CLI** (`cli.py`) con **contexto** de conversaciÃ³n. El servidor habla **JSON-RPC 2.0** por stdin/stdout (estilo MCP).

## ğŸ“‹ Requisitos

* **Python 3.10+**
* **Ollama** en local y un modelo Llama (ej: `llama3.2:3b`)
* Dependencias del sistema (solo para exportar PDF con WeasyPrint):
  * **macOS:** `brew install pango cairo gdk-pixbuf libffi`
  * **Linux:** instala paquetes equivalentes (pango, cairo, gdk-pixbuf, libffi)

## âš™ï¸ InstalaciÃ³n

```bash
git clone https://github.com/<tu-usuario>/MCP-LOCAL-SERVER.git
cd MCP-LOCAL-SERVER

python -m venv .venv
source .venv/bin/activate   # macOS/Linux
# .venv\Scripts\activate    # Windows (PowerShell)

pip install -r requirements.txt
```

## ğŸ¦™ Modelo Llama con Ollama

1. **Instala/abre Ollama y arrÃ¡ncalo:**

```bash
ollama serve
```

2. **Descarga un modelo (ejemplo de 3B):**

```bash
ollama pull llama3.2:3b
```

## ğŸ”§ ConfiguraciÃ³n

Crea un archivo `.env` (o copia de `.env.example`) con:

```env
OPENAI_API_BASE=http://localhost:11434/v1
OPENAI_API_KEY=ollama
LLM_MODEL=llama3.2:3b
LLM_SYSTEM_PROMPT_PATH=prompts/system_llm.txt
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=60
```

El **system prompt** principal estÃ¡ en `prompts/system_llm.txt` y es fÃ¡cil de modificar.

## ğŸš€ Uso rÃ¡pido

### 1) Demo end-to-end (genera reporte)

```bash
python demo.py
```

**QuÃ© hace:**
* Lee `samples/informe.pdf` y extrae texto/tablas
* Perfila `samples/datos.csv`
* Calcula forecast simple de `produccion`
* Pregunta al LLM un resumen del servidor
* Genera un **reporte** en `reports/` (HTML o PDF, segÃºn config en `demo.py`)

### 2) Chat CLI con contexto

```bash
python cli.py
```

**Comandos Ãºtiles dentro del CLI:**

```bash
/tools               # lista herramientas MCP
/new                 # reinicia el contexto de chat
/save [archivo.md]   # guarda el transcript (por defecto reports/chat.md)
/call NAME {json}    # llama cualquier tool con JSON (ej: report_generate)
/help                # ayuda
/exit                # salir
```

**Ejemplo `/call`:**

```bash
/call report_generate {"title":"Demo","sections":["Hola"],"format":"pdf"}
```

## ğŸ“Š Logs y artefactos

* **Artefactos** (reportes y grÃ¡ficos): `reports/`
* **Log MCP** (JSONL): `reports/mcp.log.jsonl`

Cada lÃ­nea incluye: `ts`, `method`, `ok`, `duration_ms`, `tool`, `args`, `result_size`, `error`.

Ver los Ãºltimos eventos:

```bash
tail -n 20 reports/mcp.log.jsonl
```

## ğŸ“‚ Estructura del proyecto

```
MCP-LOCAL-SERVER/
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ system_llm.txt
â”œâ”€â”€ reports/                 # artefactos (pdf/html/png, log jsonl)
â”œâ”€â”€ samples/
â”‚   â”œâ”€â”€ informe.pdf
â”‚   â””â”€â”€ datos.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ data_profile.py
â”‚   â”‚   â”œâ”€â”€ llm_chat.py
â”‚   â”‚   â”œâ”€â”€ pdf_extract.py
â”‚   â”‚   â”œâ”€â”€ report_generate.py
â”‚   â”‚   â””â”€â”€ ts_forecast.py
â”‚   â””â”€â”€ util/
â”‚       â”œâ”€â”€ config.py        # (si aplica)
â”‚       â”œâ”€â”€ io.py            # (si aplica)
â”‚       â””â”€â”€ registry.py
â”œâ”€â”€ .env.example             # (opcional)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ main.py                  # servidor MCP (JSON-RPC)
â”œâ”€â”€ demo.py                  # demo orquestada
â””â”€â”€ cli.py                   # chatbot CLI con contexto
```

## ğŸ”§ Troubleshooting

### **Ollama no responde / timeout**
* AsegÃºrate de tener `ollama serve` corriendo (verifica con `curl -s http://localhost:11434/api/version`).
* Verifica el modelo (`ollama list`) y el nombre en `LLM_MODEL`.

### **Falla exportaciÃ³n a PDF**
* Instala dependencias del sistema: 
  * **macOS:** `brew install pango cairo gdk-pixbuf libffi`
* Reinstala Python deps: `pip install weasyprint`

### **Permisos de archivos**
* Asegura que `reports/` exista y sea escribible (el cÃ³digo crea la carpeta si falta).

## ğŸ“„ Licencia

MIT â€” ver `LICENSE`.

## ğŸ‘¨â€ğŸ’» CrÃ©ditos

**Proyecto acadÃ©mico:** MCP Server Local en Python (Parte 1)  
**Autor:** Jorge Luis Lopez 221038